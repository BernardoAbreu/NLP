{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 2\n",
    "## Processamento de Linguagem Natural - 2018/2\n",
    "\n",
    "### Bernardo de Almeida Abreu - 2018718155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "A tarefa de realizar um *Part-of-speech tagging* (POS tagging) é uma tarefa clássica da área de processamento de linguagem natural. Ela consiste em assinalar uma classe gramatical para cada token de um texto [1]. Alguns exemplos de classes gramaticais que podem ser assinaladas aos tokens de um texto são \"substantivo\", \"adjetivo\" e \"pontuação\". Muitas vezes uma mesma palavra assume papéis e significados diferentes dependendo do contexto em que se encontra, de modo que essa tarefa não é trivial.\n",
    "\n",
    "As melhores soluções para esse problema se baseiam em técnicas de aprendizado de máquina supervisionado e, por esse motivo, é necessário que exista uma base de dados anotada na língua correta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação\n",
    "\n",
    "O objetivo desse trabalho é implementar a tarefa de POS-tagging para uma base de dados em português. A base de dados utilizada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T22:19:45.297723Z",
     "start_time": "2018-11-11T22:19:45.276119Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:03:42.563127Z",
     "start_time": "2018-11-11T20:03:42.543143Z"
    }
   },
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'train': '../macmorpho-v3/macmorpho-train.txt',\n",
    "    'test': '../macmorpho-v3/macmorpho-test.txt',\n",
    "    'dev': '../macmorpho-v3/macmorpho-dev.txt',\n",
    "    'word2vec': '../data/skip_s100.txt',\n",
    "    'model': '../src/model.json',\n",
    "    'model_weights': '../src/model.h5'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:43:32.737582Z",
     "start_time": "2018-11-11T20:43:32.728194Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_text(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:43:34.328649Z",
     "start_time": "2018-11-11T20:43:34.244288Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text = read_text(paths['train'])\n",
    "test_text = read_text(paths['test'])\n",
    "dev_text = read_text(paths['dev'])\n",
    "\n",
    "print(train_text[0])\n",
    "print(test_text[0])\n",
    "print(dev_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação de palavras e tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:43:36.440292Z",
     "start_time": "2018-11-11T20:43:36.427045Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_word_tags(text):\n",
    "    word_lines = []\n",
    "    tag_lines = []\n",
    "    for line in text:\n",
    "        words, tags = zip(*[tagged_word.split('_') for tagged_word in line.split()])\n",
    "        word_lines.append([w.lower() for w in words])\n",
    "        tag_lines.append(list(tags))\n",
    "    return word_lines, tag_lines\n",
    "\n",
    "def flat_list(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:44:08.590318Z",
     "start_time": "2018-11-11T20:44:06.440366Z"
    }
   },
   "outputs": [],
   "source": [
    "train_words, train_tags = split_word_tags(train_text)\n",
    "print(train_words[0])\n",
    "print(train_tags[0])\n",
    "\n",
    "test_words, test_tags = split_word_tags(test_text)\n",
    "dev_words, dev_tags = split_word_tags(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:45:15.831287Z",
     "start_time": "2018-11-11T20:45:15.398210Z"
    }
   },
   "outputs": [],
   "source": [
    "id2tag = ['<PAD>'] + list(set(flat_list(train_tags)).union(set(flat_list(test_tags))).union(set(flat_list(dev_tags))))\n",
    "tag2id = {}\n",
    "for i, tag in enumerate(id2tag):\n",
    "    tag2id[tag] = i\n",
    "print(tag2id)\n",
    "print(id2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding das sentenças"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise da distribuição  dos tamanhos das sentenças"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É necessário que todas as sentenças do texto possuam o mesmo tamanho para que possam ser fornecidas de entrada para a rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:45:37.004012Z",
     "start_time": "2018-11-11T20:45:36.897267Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(columns=['words', 'tags'])\n",
    "df_test = pd.DataFrame(columns=['words', 'tags'])\n",
    "df_dev = pd.DataFrame(columns=['words', 'tags'])\n",
    "\n",
    "df_train['words'] = train_words\n",
    "df_train['tags'] = train_tags\n",
    "\n",
    "df_test['words'] = test_words\n",
    "df_test['tags'] = test_tags\n",
    "\n",
    "df_dev['words'] = dev_words\n",
    "df_dev['tags'] = dev_tags\n",
    "\n",
    "df_sentences = pd.concat([df_train, df_test, df_dev], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:45:39.450723Z",
     "start_time": "2018-11-11T20:45:39.337810Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sentences['words'].map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:45:42.241958Z",
     "start_time": "2018-11-11T20:45:42.005442Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sentences['words'].map(len).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:45:47.025275Z",
     "start_time": "2018-11-11T20:45:46.923548Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = int(df_sentences['words'].map(len).describe()['75%'])\n",
    "MAX_SENTENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:45:48.569695Z",
     "start_time": "2018-11-11T20:45:48.557855Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_sentence(sentence):\n",
    "    tokens_to_fill = int(MAX_SENTENCE_LENGTH - len(sentence))\n",
    "    \n",
    "#     sentence.append('<END>')\n",
    "    sentence.extend(['<PAD>']*tokens_to_fill)\n",
    "    \n",
    "    return sentence[:MAX_SENTENCE_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:45:52.030989Z",
     "start_time": "2018-11-11T20:45:51.527260Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train[\"words\"] = df_train[\"words\"].map(fill_sentence)\n",
    "df_train[\"tags\"] = df_train[\"tags\"].map(fill_sentence)\n",
    "\n",
    "df_test[\"words\"] = df_test[\"words\"].map(fill_sentence)\n",
    "df_test[\"tags\"] = df_test[\"tags\"].map(fill_sentence)\n",
    "\n",
    "df_dev[\"words\"] = df_dev[\"words\"].map(fill_sentence)\n",
    "df_dev[\"tags\"] = df_dev[\"tags\"].map(fill_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:45:54.417492Z",
     "start_time": "2018-11-11T20:45:54.385848Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:45:56.159230Z",
     "start_time": "2018-11-11T20:45:56.134694Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dev['words'].map(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:34:49.912752Z",
     "start_time": "2018-11-11T20:03:45.914274Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(paths['word2vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:42:23.377836Z",
     "start_time": "2018-11-11T20:42:22.189410Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_model.similar_by_vector('hemocentro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adiciona vetores extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:42:32.888793Z",
     "start_time": "2018-11-11T20:42:32.376028Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model.add(['<PAD>','<OOV>'], [[0.1]*100,[0.2]*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:43:27.189931Z",
     "start_time": "2018-11-11T20:43:27.176644Z"
    }
   },
   "outputs": [],
   "source": [
    "print('<PAD>' in w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:53:37.763507Z",
     "start_time": "2018-11-11T20:53:37.744391Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(w2v_model.vocab))\n",
    "print(MAX_SENTENCE_LENGTH)\n",
    "print(len(df_train))\n",
    "w2v_model.vocab['<OOV>'].index\n",
    "print(len(df_train['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T21:02:45.533196Z",
     "start_time": "2018-11-11T21:02:45.511168Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_weights = w2v_model.vectors\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "\n",
    "def word2idx(word):\n",
    "    return w2v_model.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return w2v_model.index2word[idx]\n",
    "\n",
    "\n",
    "def prepare_words(sentences):\n",
    "    sentences_x = np.zeros([len(sentences), MAX_SENTENCE_LENGTH], dtype=np.int32)\n",
    "\n",
    "    oov_index = word2idx('<OOV>')\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, word in enumerate(sentence):\n",
    "            try:\n",
    "                sentences_x[i, t] = word2idx(word)\n",
    "            except KeyError:\n",
    "                sentences_x[i, t] = oov_index\n",
    "    return sentences_x\n",
    "\n",
    "def prepare_tags(tag_sentences, tag2index):\n",
    "    tags_y = np.zeros([len(tag_sentences), MAX_SENTENCE_LENGTH], dtype=np.int32)\n",
    "    for i, sentence in enumerate(tag_sentences):\n",
    "        for t, tag in enumerate(sentence):\n",
    "            tags_y[i, t] = tag2index[tag]\n",
    "    return tags_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T21:23:53.335029Z",
     "start_time": "2018-11-11T21:23:50.242324Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\nPreparing the train data for LSTM...')\n",
    "train_sentences_X = prepare_words(df_train['words'])\n",
    "print('train_x shape:', train_sentences_X.shape)\n",
    "\n",
    "print('\\nPreparing the test data for LSTM...')\n",
    "test_sentences_X = prepare_words(df_test['words'])\n",
    "print('test_x shape:', test_sentences_X.shape)\n",
    "\n",
    "print('\\nPreparing the validation data for LSTM...')\n",
    "dev_sentences_X = prepare_words(df_dev['words'])\n",
    "print('dev_x shape:', dev_sentences_X.shape)\n",
    "\n",
    "\n",
    "print('\\nPreparing the train tags for LSTM...')\n",
    "train_tags_y = prepare_tags(df_train['tags'], tag2id)\n",
    "print('train_y shape:', train_tags_y.shape)\n",
    "\n",
    "print('\\nPreparing the test data for LSTM...')\n",
    "test_tags_y = prepare_tags(df_test['tags'], tag2id)\n",
    "print('test_y shape:', test_tags_y.shape)\n",
    "\n",
    "print('\\nPreparing the validation data for LSTM...')\n",
    "dev_tags_y = prepare_tags(df_dev['tags'], tag2id)\n",
    "print('dev_y shape:', dev_tags_y.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(dev_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])\n",
    "print(dev_tags_y[0])\n",
    "\n",
    "cat_train_tags_y = keras.utils.to_categorical(train_tags_y, num_classes=len(id2tag), dtype='int32')\n",
    "cat_test_tags_y = keras.utils.to_categorical(test_tags_y, num_classes=len(id2tag), dtype='int32')\n",
    "cat_dev_tags_y = keras.utils.to_categorical(dev_tags_y, num_classes=len(id2tag), dtype='int32')\n",
    "\n",
    "# print(cat_train_tags_y[0])\n",
    "print(cat_test_tags_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T21:04:22.126197Z",
     "start_time": "2018-11-11T21:04:22.114902Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adiciona camada de embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T21:04:48.658551Z",
     "start_time": "2018-11-11T21:04:46.418260Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.add(\n",
    "    keras.layers.Embedding(\n",
    "        input_dim=len(w2v_model.vocab),\n",
    "        output_dim=emdedding_size,\n",
    "        input_length=MAX_SENTENCE_LENGTH,\n",
    "        weights=[pretrained_weights]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T21:05:21.933656Z",
     "start_time": "2018-11-11T21:05:20.803240Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM( return_sequences=True)))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(len(tag2id))))\n",
    "model.add(keras.layers.Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T22:26:35.678825Z",
     "start_time": "2018-11-11T22:26:28.149846Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40,\n",
    "          validation_data=(dev_sentences_X,cat_dev_tags_y),\n",
    "          callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_sentences_X, cat_test_tags_y)\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 99.09751977804825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### serialize model to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T22:23:34.238805Z",
     "start_time": "2018-11-11T22:23:34.229789Z"
    }
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### serialize weights to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load json and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(paths['model'], 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load weights into new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights(paths['model_weights'])\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate loaded model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = loaded_model.evaluate(train_sentences_X, cat_train_tags_y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_X = np.loadtxt('../src/train_x')\n",
    "train_tags_y = np.loadtxt('../src/train_y')\n",
    "test_sentences_X = np.loadtxt('../src/test_x')\n",
    "test_tags_y = np.loadtxt('../src/test_y')\n",
    "dev_sentences_X = np.loadtxt('../src/dev_x')\n",
    "dev_tags_y = np.loadtxt('../src/dev_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_tags_y = keras.utils.to_categorical(train_tags_y)\n",
    "cat_test_tags_y = keras.utils.to_categorical(test_tags_y)\n",
    "cat_dev_tags_y = keras.utils.to_categorical(dev_tags_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../saved_model/keras_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = keras.backend.get_session()\n",
    "saver.restore(sess, '../saved_model/keras_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluating model:')\n",
    "scores = model.evaluate(test_sentences_X, test_tags_y)\n",
    "print(\"Test model %s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n",
    "scores = model.evaluate(train_sentences_X, train_tags_y)\n",
    "print(\"Train model %s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n",
    "scores = model.evaluate(dev_sentences_X, dev_tags_y)\n",
    "print(\"Dev model %s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "1. https://arxiv.org/pdf/1508.01991.pdf\n",
    "2. http://www.aclweb.org/anthology/Y/Y09/Y09-1013.pdf\n",
    "3. https://nlpforhackers.io/lstm-pos-tagger-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
