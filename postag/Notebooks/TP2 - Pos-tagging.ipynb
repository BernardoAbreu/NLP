{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 2\n",
    "## Processamento de Linguagem Natural - 2018/2\n",
    "\n",
    "### Bernardo de Almeida Abreu - 2018718155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "A tarefa de realizar um *Part-of-speech tagging* (POS tagging) é uma tarefa clássica da área de processamento de linguagem natural. Ela consiste em assinalar uma classe gramatical para cada token de um texto [1]. Alguns exemplos de classes gramaticais que podem ser atribuídas aos tokens de um texto são \"substantivo\", \"adjetivo\" e \"pontuação\". Muitas vezes uma mesma palavra assume papéis e significados diferentes dependendo do contexto em que se encontra, de modo que essa tarefa não é trivial.\n",
    "\n",
    "As melhores soluções para esse problema se baseiam em técnicas de aprendizado de máquina supervisionado e, por esse motivo, é necessário que exista uma base de dados anotada na língua correta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação\n",
    "\n",
    "O objetivo desse trabalho é implementar a tarefa de POS-tagging para uma base de dados em português. O corpus utilizado foir o  Mac-Morpho, produzido pelo grupo NILC em da ICMC USP [3]. Essa tarefa foi implementada utilizando uma rede neural para classificar os tokens do texto da base de dados em diferentes classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:07:45.320139Z",
     "start_time": "2018-11-23T22:07:45.296854Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernardo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:07:47.644939Z",
     "start_time": "2018-11-23T22:07:47.634141Z"
    }
   },
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'train': '../macmorpho-v3/macmorpho-train.txt',\n",
    "    'test': '../macmorpho-v3/macmorpho-test.txt',\n",
    "    'dev': '../macmorpho-v3/macmorpho-dev.txt',\n",
    "    'word2vec': '../data/skip_s100.txt',\n",
    "    'word2vecpickle': '../src/word2vec_model_skipgram_100.p',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura do texto\n",
    "O corpus utilzado possui três textos anotados. Cada um deles possui o seu próprio propósito - treino, teste e validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:07:49.557436Z",
     "start_time": "2018-11-23T22:07:49.547589Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_text(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:07:50.903511Z",
     "start_time": "2018-11-23T22:07:50.759482Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jersei_N atinge_V média_N de_PREP Cr$_CUR 1,4_NUM milhão_N na_PREP+ART venda_N da_PREP+ART Pinhal_NPROP em_PREP São_NPROP Paulo_NPROP ._PU\n",
      "\n",
      "Salto_N sete_ADJ\n",
      "\n",
      "Ainda_ADV em_PREP dezembro_N de_PREP 1990_N ,_PU foi_V editada_PCP a_ART famosa_ADJ 289_N ,_PU que_PRO-KS modificava_V a_ART sistemática_N da_PREP+ART arrecadação_N do_PREP+ART ITR_NPROP e_KC alterava_V suas_PROADJ alíquotas_N ._PU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_text = read_text(paths['train'])\n",
    "test_text = read_text(paths['test'])\n",
    "dev_text = read_text(paths['dev'])\n",
    "\n",
    "print(train_text[0])\n",
    "print(test_text[0])\n",
    "print(dev_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação de palavras e tags\n",
    "Cada token nos arquivos possui uma *tag* relacionada ao mesmo. Para que se possa fazer o treinamento do modelo de POS-tagging é necessário separar cada token de sua respectiva *tag*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:07:52.767813Z",
     "start_time": "2018-11-23T22:07:52.754267Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_word_tags(text):\n",
    "    word_lines = []\n",
    "    tag_lines = []\n",
    "    for line in text:\n",
    "        words, tags = zip(*[tagged_word.split('_') for tagged_word in line.split()])\n",
    "        word_lines.append([w.lower() for w in words])\n",
    "        tag_lines.append(list(tags))\n",
    "    return word_lines, tag_lines\n",
    "\n",
    "def flat_list(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:07:56.221024Z",
     "start_time": "2018-11-23T22:07:54.212085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jersei', 'atinge', 'média', 'de', 'cr$', '1,4', 'milhão', 'na', 'venda', 'da', 'pinhal', 'em', 'são', 'paulo', '.']\n",
      "['N', 'V', 'N', 'PREP', 'CUR', 'NUM', 'N', 'PREP+ART', 'N', 'PREP+ART', 'NPROP', 'PREP', 'NPROP', 'NPROP', 'PU']\n"
     ]
    }
   ],
   "source": [
    "train_words, train_tags = split_word_tags(train_text)\n",
    "print(train_words[0])\n",
    "print(train_tags[0])\n",
    "\n",
    "test_words, test_tags = split_word_tags(test_text)\n",
    "dev_words, dev_tags = split_word_tags(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:08:21.615161Z",
     "start_time": "2018-11-23T22:08:21.371417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', 'ADJ', 'ADV', 'ADV-KS', 'ART', 'CUR', 'IN', 'KC', 'KS', 'N', 'NPROP', 'NUM', 'PCP', 'PDEN', 'PREP', 'PREP+ADV', 'PREP+ART', 'PREP+PRO-KS', 'PREP+PROADJ', 'PREP+PROPESS', 'PREP+PROSUB', 'PRO-KS', 'PROADJ', 'PROPESS', 'PROSUB', 'PU', 'V']\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "id2tag = ['<PAD>'] + sorted(list(set(flat_list(train_tags)).union(set(flat_list(test_tags))).union(set(flat_list(dev_tags)))))\n",
    "tag2id = {}\n",
    "for i, tag in enumerate(id2tag):\n",
    "    tag2id[tag] = i\n",
    "\n",
    "print(id2tag)\n",
    "print(len(id2tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:08:23.241022Z",
     "start_time": "2018-11-23T22:08:23.075553Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(columns=['words', 'tags'])\n",
    "df_test = pd.DataFrame(columns=['words', 'tags'])\n",
    "df_dev = pd.DataFrame(columns=['words', 'tags'])\n",
    "\n",
    "df_train['words'] = train_words\n",
    "df_train['tags'] = train_tags\n",
    "\n",
    "df_test['words'] = test_words\n",
    "df_test['tags'] = test_tags\n",
    "\n",
    "df_dev['words'] = dev_words\n",
    "df_dev['tags'] = dev_tags\n",
    "\n",
    "df_sentences = pd.concat([df_train, df_test, df_dev], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding das sentenças\n",
    "A entrada da rede neural deve ser de um tamanho fixo. Para que as diferentes sentenças presentes no corpus, que podem possuir tamanhos distintos entre si, possam ser fornecidas como entrada para a rede, é necessário padronizar o tamanho das mesmas. Essa padronização é feita através de dois passos, remover as palavras extras da sentenças que ultrapassam o tamanho definido, e adicionar um token de *padding* às sentenças que possuem um comprimento menor do que o tamanho determinado, que é então repetido até que a sentença atinja o tamanho correto.\n",
    "\n",
    "Esse tamanho foi determinado de acordo com a distribuição dos tamanhos das sentenças ao longo do corpus. Para obter um compromisso entre sentenças com muito *padding* e sentenças que devem ter palavras removidas, o tamanho foi definido como aquele que evita eliminar palavras de 75% de todas as sentenças do corpus, incluindo as bases de treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:08:34.479000Z",
     "start_time": "2018-11-23T22:08:34.366856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49932.000000\n",
       "mean        18.940779\n",
       "std         12.070051\n",
       "min          1.000000\n",
       "25%         10.000000\n",
       "50%         17.000000\n",
       "75%         25.000000\n",
       "max        248.000000\n",
       "Name: words, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences['words'].map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:08:37.288760Z",
     "start_time": "2018-11-23T22:08:37.087179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd994a94908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sentences['words'].map(len).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:08:46.375102Z",
     "start_time": "2018-11-23T22:08:46.256881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = int(df_sentences['words'].map(len).describe()['75%'])\n",
    "MAX_SENTENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:08:47.626404Z",
     "start_time": "2018-11-23T22:08:47.615066Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_sentence(sentence):\n",
    "    tokens_to_fill = int(MAX_SENTENCE_LENGTH - len(sentence))\n",
    "    sentence.extend(['<PAD>']*tokens_to_fill)\n",
    "    \n",
    "    return sentence[:MAX_SENTENCE_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:08:50.630590Z",
     "start_time": "2018-11-23T22:08:49.886609Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train[\"words\"] = df_train[\"words\"].map(fill_sentence)\n",
    "df_train[\"tags\"] = df_train[\"tags\"].map(fill_sentence)\n",
    "\n",
    "df_test[\"words\"] = df_test[\"words\"].map(fill_sentence)\n",
    "df_test[\"tags\"] = df_test[\"tags\"].map(fill_sentence)\n",
    "\n",
    "df_dev[\"words\"] = df_dev[\"words\"].map(fill_sentence)\n",
    "df_dev[\"tags\"] = df_dev[\"tags\"].map(fill_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:08:51.991026Z",
     "start_time": "2018-11-23T22:08:51.841126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[salto, sete, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD...</td>\n",
       "      <td>[N, ADJ, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;, &lt;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[o, grande, assunto, da, semana, em, nova, yor...</td>\n",
       "      <td>[ART, ADJ, N, PREP+ART, N, PREP, NPROP, NPROP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[número, duplo, especial, ,, é, inteirinho, de...</td>\n",
       "      <td>[N, ADJ, ADJ, PU, V, ADJ, PCP, PREP, N, PREP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a, endiabrada, editora, tina, brown, ex, da, ...</td>\n",
       "      <td>[ART, PCP, N, NPROP, NPROP, N, PREP+ART, PU, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[além, das, fotos, de, richard, avedon, ., &lt;PA...</td>\n",
       "      <td>[PREP, PREP+ART, N, PREP, NPROP, NPROP, PU, &lt;P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0  [salto, sete, <PAD>, <PAD>, <PAD>, <PAD>, <PAD...   \n",
       "1  [o, grande, assunto, da, semana, em, nova, yor...   \n",
       "2  [número, duplo, especial, ,, é, inteirinho, de...   \n",
       "3  [a, endiabrada, editora, tina, brown, ex, da, ...   \n",
       "4  [além, das, fotos, de, richard, avedon, ., <PA...   \n",
       "\n",
       "                                                tags  \n",
       "0  [N, ADJ, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <P...  \n",
       "1  [ART, ADJ, N, PREP+ART, N, PREP, NPROP, NPROP,...  \n",
       "2  [N, ADJ, ADJ, PU, V, ADJ, PCP, PREP, N, PREP, ...  \n",
       "3  [ART, PCP, N, NPROP, NPROP, N, PREP+ART, PU, N...  \n",
       "4  [PREP, PREP+ART, N, PREP, NPROP, NPROP, PU, <P...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:08:53.690689Z",
     "start_time": "2018-11-23T22:08:53.661248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1997.0\n",
       "mean       25.0\n",
       "std         0.0\n",
       "min        25.0\n",
       "25%        25.0\n",
       "50%        25.0\n",
       "75%        25.0\n",
       "max        25.0\n",
       "Name: words, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['words'].map(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding - Word2Vec\n",
    "A representação das palavras na rede neural foi feita através do modelo word2vec. Para isso, foi escolhido um embedding já treinado em português, produzido pelo grupo NILC em da ICMC USP [4] O embedding escolhido foi treinado através do skip-gram e possui vetores de tamanho 100 para cada palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pickle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:34:49.912752Z",
     "start_time": "2018-11-11T20:03:45.914274Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_pickle:\n",
    "    w2v_model = pickle.load(open(paths['word2vecpickle'], 'rb'))\n",
    "else:\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(paths['word2vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:42:23.377836Z",
     "start_time": "2018-11-11T20:42:22.189410Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernardo/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hemopa', 0.8358802199363708),\n",
       " ('hemonúcleo', 0.8148629069328308),\n",
       " ('hemoal', 0.7524208426475525),\n",
       " ('hemorio', 0.7506005167961121),\n",
       " ('procon', 0.7349455952644348),\n",
       " ('centrinho', 0.7315341234207153),\n",
       " ('hemoam', 0.7078859806060791),\n",
       " ('incor', 0.7009121775627136),\n",
       " ('detran', 0.7000017166137695),\n",
       " ('poupatempo', 0.6961973309516907)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.similar_by_vector('hemocentro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adiciona vetores extras\n",
    "Dois novos tokens precisaram ser adicionados ao embedding. O token `<PAD>` é o token utilizado para realizar o *padding* nas sentenças, enquanto o token `<OOV>` é utilizado para substituir nas sentenças as palavras que não puderam ser encontradas no embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:42:32.888793Z",
     "start_time": "2018-11-11T20:42:32.376028Z"
    }
   },
   "outputs": [],
   "source": [
    "#w2v_model.add(['<PAD>','<OOV>'], [[0.1]*100,[0.2]*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:43:27.189931Z",
     "start_time": "2018-11-11T20:43:27.176644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('<PAD>' in w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T21:02:45.533196Z",
     "start_time": "2018-11-11T21:02:45.511168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (929608, 100)\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = w2v_model.vectors\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "\n",
    "def word2idx(word):\n",
    "    return w2v_model.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return w2v_model.index2word[idx]\n",
    "\n",
    "\n",
    "def prepare_words(sentences):\n",
    "    sentences_x = np.zeros([len(sentences), MAX_SENTENCE_LENGTH], dtype=np.int32)\n",
    "\n",
    "    oov_index = word2idx('<OOV>')\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, word in enumerate(sentence):\n",
    "            try:\n",
    "                sentences_x[i, t] = word2idx(word)\n",
    "            except KeyError:\n",
    "                sentences_x[i, t] = oov_index\n",
    "    return sentences_x\n",
    "\n",
    "def prepare_tags(tag_sentences, tag2index):\n",
    "    tags_y = np.zeros([len(tag_sentences), MAX_SENTENCE_LENGTH], dtype=np.int32)\n",
    "    for i, sentence in enumerate(tag_sentences):\n",
    "        for t, tag in enumerate(sentence):\n",
    "            tags_y[i, t] = tag2index[tag]\n",
    "    return tags_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T21:23:53.335029Z",
     "start_time": "2018-11-11T21:23:50.242324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the train data for LSTM...\n",
      "train_x shape: (37948, 25)\n",
      "\n",
      "Preparing the test data for LSTM...\n",
      "test_x shape: (9987, 25)\n",
      "\n",
      "Preparing the validation data for LSTM...\n",
      "dev_x shape: (1997, 25)\n",
      "\n",
      "Preparing the train tags for LSTM...\n",
      "train_y shape: (37948, 25)\n",
      "\n",
      "Preparing the test data for LSTM...\n",
      "test_y shape: (9987, 25)\n",
      "\n",
      "Preparing the validation data for LSTM...\n",
      "dev_y shape: (1997, 25)\n"
     ]
    }
   ],
   "source": [
    "print('\\nPreparing the train data for LSTM...')\n",
    "train_sentences_X = prepare_words(df_train['words'])\n",
    "print('train_x shape:', train_sentences_X.shape)\n",
    "\n",
    "print('\\nPreparing the test data for LSTM...')\n",
    "test_sentences_X = prepare_words(df_test['words'])\n",
    "print('test_x shape:', test_sentences_X.shape)\n",
    "\n",
    "print('\\nPreparing the validation data for LSTM...')\n",
    "dev_sentences_X = prepare_words(df_dev['words'])\n",
    "print('dev_x shape:', dev_sentences_X.shape)\n",
    "\n",
    "\n",
    "print('\\nPreparing the train tags for LSTM...')\n",
    "train_tags_y = prepare_tags(df_train['tags'], tag2id)\n",
    "print('train_y shape:', train_tags_y.shape)\n",
    "\n",
    "print('\\nPreparing the test data for LSTM...')\n",
    "test_tags_y = prepare_tags(df_test['tags'], tag2id)\n",
    "print('test_y shape:', test_tags_y.shape)\n",
    "\n",
    "print('\\nPreparing the validation data for LSTM...')\n",
    "dev_tags_y = prepare_tags(df_dev['tags'], tag2id)\n",
    "print('dev_y shape:', dev_tags_y.shape)\n",
    "\n",
    "cat_train_tags_y = keras.utils.to_categorical(train_tags_y, num_classes=len(id2tag), dtype='int32')\n",
    "cat_test_tags_y = keras.utils.to_categorical(test_tags_y, num_classes=len(id2tag), dtype='int32')\n",
    "cat_dev_tags_y = keras.utils.to_categorical(dev_tags_y, num_classes=len(id2tag), dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitetura do modelo\n",
    "Redes LSTM bidirecionais são redes LSTM que fazem uma passada em cada direção da sequência de entrada, antes de passar para a próxima camada [5]. Isso permite que a rede leve em consideração tanto o contexto dos tokens que vem antes do token que se quer classificar, quanto o contexto dos tokens que aparecem depois. Esse tipo de rede funciona bem para tarefas de POS tagging com poucas sentenas de treinamento.\n",
    "\n",
    "A arquitetura definida para esse projeto foi uma camada de embedding que utiliza word2vec, seguido de uma camada LSTM bidirecional e uma camada densa.\n",
    "\n",
    "As tags de `<PAD>` não são relevantes para o aprendizado das outras tags, além disso, são fáceis de se acertar. Assim, foi criada uma métrica que ignora a acurácia da tag `<PAD>` durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = keras.backend.argmax(y_true, axis=-1)\n",
    "        y_pred_class = keras.backend.argmax(y_pred, axis=-1)\n",
    "        ignore_mask = keras.backend.cast(\n",
    "            keras.backend.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = keras.backend.cast(\n",
    "            keras.backend.equal(y_true_class, y_pred_class), 'int32') * \\\n",
    "            ignore_mask\n",
    "        accuracy = keras.backend.sum(matches) / \\\n",
    "            keras.backend.maximum(keras.backend.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação da arquitetura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T21:04:22.126197Z",
     "start_time": "2018-11-11T21:04:22.114902Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T21:04:48.658551Z",
     "start_time": "2018-11-11T21:04:46.418260Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.add(\n",
    "#     keras.layers.Embedding(\n",
    "#         input_dim=len(w2v_model.vocab),\n",
    "#         output_dim=emdedding_size,\n",
    "#         input_length=MAX_SENTENCE_LENGTH,\n",
    "#         weights=[pretrained_weights]\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T21:05:21.933656Z",
     "start_time": "2018-11-11T21:05:20.803240Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.add(\n",
    "#     keras.layers.Bidirectional(\n",
    "#         keras.layers.LSTM(lstm_size, return_sequences=True)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# model.add(\n",
    "#     keras.layers.TimeDistributed(\n",
    "#         keras.layers.Dense(output_len)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# model.add(keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=\"adam\",\n",
    "#               metrics=['accuracy', ignore_class_accuracy(tag2id['<PAD>'])])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T22:26:35.678825Z",
     "start_time": "2018-11-11T22:26:28.149846Z"
    }
   },
   "outputs": [],
   "source": [
    "# csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "#                                            min_delta=0.001,\n",
    "#                                            patience=4,\n",
    "#                                            verbose=1,\n",
    "#                                            mode='min')\n",
    "# model.fit(train_sentences_X, cat_train_tags_y,\n",
    "#           batch_size=64, epochs=5,\n",
    "#           validation_data=(dev_sentences_X, cat_dev_tags_y),\n",
    "#           callbacks=[csv_logger, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = model.evaluate(test_X, test_Y)\n",
    "# for metric, score in zip(model.metrics_names, scores):\n",
    "#     print(f\"Test model {metric}: {score*100}\")\n",
    "\n",
    "# scores = model.evaluate(train_X, train_Y)\n",
    "# for metric, score in zip(model.metrics_names, scores):\n",
    "#     print(f\"Train model {metric}: {score*100}\")\n",
    "\n",
    "# scores = model.evaluate(dev_X, dev_Y)\n",
    "# for metric, score in zip(model.metrics_names, scores):\n",
    "#     print(f\"Dev model {metric}: {score*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>ignore_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025694</td>\n",
       "      <td>0.992647</td>\n",
       "      <td>0.988631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.106946</td>\n",
       "      <td>0.969977</td>\n",
       "      <td>0.948812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.096085</td>\n",
       "      <td>0.972719</td>\n",
       "      <td>0.958720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  ignore_accuracy\n",
       "0  0.025694  0.992647         0.988631\n",
       "1  0.106946  0.969977         0.948812\n",
       "2  0.096085  0.972719         0.958720"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_data = {'train':0, 'test':1, 'dev':2}\n",
    "scores = pd.read_csv('../src/scores.txt')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "[ 4 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "[ 4 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n"
     ]
    }
   ],
   "source": [
    "test_pred = np.loadtxt('../src/test_predict.txt').astype(int)\n",
    "test_tags_y2 = np.loadtxt('../src/test_y').astype(int)\n",
    "print(test_pred[0])\n",
    "print(test_tags_y[0])\n",
    "print(test_tags_y2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "print(tag2id['ADJ'])\n",
    "print(id2tag[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <PAD>       1.00      1.00      1.00     95832\n",
      "          KC       0.97      0.98      0.98      3695\n",
      "         NUM       0.00      0.00      0.00     13261\n",
      "        PDEN       0.00      0.00      0.00       165\n",
      "           N       0.95      0.94      0.95     31694\n",
      "PREP+PROPESS       0.00      0.00      0.00     11371\n",
      "          IN       0.00      0.00      0.00       106\n",
      "         PCP       0.00      0.00      0.00       136\n",
      "      PRO-KS       0.00      0.00      0.00        26\n",
      "    PREP+ART       0.00      0.00      0.00       287\n",
      "         CUR       0.00      0.00      0.00      4792\n",
      "    PREP+ADV       0.00      0.00      0.00      1362\n",
      "      PROADJ       0.01      0.01      0.01      2382\n",
      "         ART       0.00      0.00      0.00       264\n",
      "          KS       0.00      0.05      0.00       131\n",
      "     PROPESS       0.00      0.00      0.00      2242\n",
      "         ADV       0.01      0.01      0.01      7148\n",
      " PREP+PRO-KS       0.00      0.00      0.00       843\n",
      " PREP+PROSUB       0.00      0.00      0.00        38\n",
      "          PU       0.00      0.00      0.00      2926\n",
      "           V       0.00      0.00      0.00      8831\n",
      "         ADJ       0.00      0.00      0.00      1880\n",
      " PREP+PROADJ       0.00      0.00      0.00      3052\n",
      "        PREP       0.00      0.00      0.00     22119\n",
      "       NPROP       0.01      0.01      0.01     14732\n",
      "      PROSUB       0.00      0.00      0.00      2499\n",
      "      ADV-KS       0.00      0.00      0.00     17861\n",
      "\n",
      " avg / total       0.52      0.52      0.52    249675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_pred.flatten(), test_tags_y.flatten(), target_names=id2tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "1. https://arxiv.org/pdf/1508.01991.pdf\n",
    "2. http://www.aclweb.org/anthology/Y/Y09/Y09-1013.pdf\n",
    "3. http://nilc.icmc.usp.br/macmorpho/\n",
    "4. http://nilc.icmc.usp.br/embeddings\n",
    "5. http://www.aclweb.org/anthology/P16-2067\n",
    "6. https://nlpforhackers.io/lstm-pos-tagger-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
