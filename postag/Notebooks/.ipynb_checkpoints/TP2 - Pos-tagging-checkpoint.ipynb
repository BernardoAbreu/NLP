{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 2\n",
    "## Processamento de Linguagem Natural - 2018/2\n",
    "\n",
    "### Bernardo de Almeida Abreu - 2018718155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "A tarefa de realizar um *Part-of-speech tagging* (POS tagging) é uma tarefa clássica da área de processamento de linguagem natural. Ela consiste em assinalar uma classe gramatical para cada token de um texto [1]. Alguns exemplos de classes gramaticais que podem ser atribuídas aos tokens de um texto são \"substantivo\", \"adjetivo\" e \"pontuação\". Muitas vezes uma mesma palavra assume papéis e significados diferentes dependendo do contexto em que se encontra, de modo que essa tarefa não é trivial.\n",
    "\n",
    "As melhores soluções para esse problema se baseiam em técnicas de aprendizado de máquina supervisionado e, por esse motivo, é necessário que exista uma base de dados anotada na língua correta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação\n",
    "\n",
    "O objetivo desse trabalho é implementar a tarefa de POS-tagging para uma base de dados em português. O corpus utilizado foir o  Mac-Morpho, produzido pelo grupo NILC em da ICMC USP [3]. Essa tarefa foi implementada utilizando uma rede neural para classificar os tokens do texto da base de dados em diferentes classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:25.507630Z",
     "start_time": "2018-11-25T05:05:08.092398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:25.521395Z",
     "start_time": "2018-11-25T05:05:25.511833Z"
    }
   },
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'train': '../macmorpho-v3/macmorpho-train.txt',\n",
    "    'test': '../macmorpho-v3/macmorpho-test.txt',\n",
    "    'dev': '../macmorpho-v3/macmorpho-dev.txt',\n",
    "    'word2vec': '../data/skip_s100.txt',\n",
    "    'word2vecpickle': '../src/word2vec_model_skipgram_100.p',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura do texto\n",
    "O corpus utilzado possui três textos anotados. Cada um deles possui o seu próprio propósito - treino, teste e validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:25.561616Z",
     "start_time": "2018-11-25T05:05:25.527544Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_text(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:25.767184Z",
     "start_time": "2018-11-25T05:05:25.578377Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jersei_N atinge_V média_N de_PREP Cr$_CUR 1,4_NUM milhão_N na_PREP+ART venda_N da_PREP+ART Pinhal_NPROP em_PREP São_NPROP Paulo_NPROP ._PU\n",
      "\n",
      "Salto_N sete_ADJ\n",
      "\n",
      "Ainda_ADV em_PREP dezembro_N de_PREP 1990_N ,_PU foi_V editada_PCP a_ART famosa_ADJ 289_N ,_PU que_PRO-KS modificava_V a_ART sistemática_N da_PREP+ART arrecadação_N do_PREP+ART ITR_NPROP e_KC alterava_V suas_PROADJ alíquotas_N ._PU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_text = read_text(paths['train'])\n",
    "test_text = read_text(paths['test'])\n",
    "dev_text = read_text(paths['dev'])\n",
    "\n",
    "print(train_text[0])\n",
    "print(test_text[0])\n",
    "print(dev_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação de palavras e tags\n",
    "Cada token nos arquivos possui uma *tag* relacionada ao mesmo. Para que se possa fazer o treinamento do modelo de POS-tagging é necessário separar cada token de sua respectiva *tag*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:25.804177Z",
     "start_time": "2018-11-25T05:05:25.793005Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_word_tags(text):\n",
    "    word_lines = []\n",
    "    tag_lines = []\n",
    "    for line in text:\n",
    "        words, tags = zip(*[tagged_word.split('_') for tagged_word in line.split()])\n",
    "        word_lines.append([w.lower() for w in words])\n",
    "        tag_lines.append(list(tags))\n",
    "    return word_lines, tag_lines\n",
    "\n",
    "def flat_list(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:28.035450Z",
     "start_time": "2018-11-25T05:05:25.814485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jersei', 'atinge', 'média', 'de', 'cr$', '1,4', 'milhão', 'na', 'venda', 'da', 'pinhal', 'em', 'são', 'paulo', '.']\n",
      "['N', 'V', 'N', 'PREP', 'CUR', 'NUM', 'N', 'PREP+ART', 'N', 'PREP+ART', 'NPROP', 'PREP', 'NPROP', 'NPROP', 'PU']\n"
     ]
    }
   ],
   "source": [
    "train_words, train_tags = split_word_tags(train_text)\n",
    "print(train_words[0])\n",
    "print(train_tags[0])\n",
    "\n",
    "test_words, test_tags = split_word_tags(test_text)\n",
    "dev_words, dev_tags = split_word_tags(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:28.319462Z",
     "start_time": "2018-11-25T05:05:28.038548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', 'ADJ', 'ADV', 'ADV-KS', 'ART', 'CUR', 'IN', 'KC', 'KS', 'N', 'NPROP', 'NUM', 'PCP', 'PDEN', 'PREP', 'PREP+ADV', 'PREP+ART', 'PREP+PRO-KS', 'PREP+PROADJ', 'PREP+PROPESS', 'PREP+PROSUB', 'PRO-KS', 'PROADJ', 'PROPESS', 'PROSUB', 'PU', 'V']\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "id2tag = ['<PAD>'] + sorted(list(set(flat_list(train_tags)).union(set(flat_list(test_tags))).union(set(flat_list(dev_tags)))))\n",
    "tag2id = {}\n",
    "for i, tag in enumerate(id2tag):\n",
    "    tag2id[tag] = i\n",
    "\n",
    "print(id2tag)\n",
    "print(len(id2tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:28.481478Z",
     "start_time": "2018-11-25T05:05:28.322617Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(columns=['words', 'tags'])\n",
    "df_test = pd.DataFrame(columns=['words', 'tags'])\n",
    "df_dev = pd.DataFrame(columns=['words', 'tags'])\n",
    "\n",
    "df_train['words'] = train_words\n",
    "df_train['tags'] = train_tags\n",
    "\n",
    "df_test['words'] = test_words\n",
    "df_test['tags'] = test_tags\n",
    "\n",
    "df_dev['words'] = dev_words\n",
    "df_dev['tags'] = dev_tags\n",
    "\n",
    "df_sentences = pd.concat([df_train, df_test, df_dev], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding das sentenças\n",
    "A entrada da rede neural deve ser de um tamanho fixo. Para que as diferentes sentenças presentes no corpus, que podem possuir tamanhos distintos entre si, possam ser fornecidas como entrada para a rede, é necessário padronizar o tamanho das mesmas. Essa padronização é feita através de dois passos, remover as palavras extras da sentenças que ultrapassam o tamanho definido, e adicionar um token de *padding* às sentenças que possuem um comprimento menor do que o tamanho determinado, que é então repetido até que a sentença atinja o tamanho correto.\n",
    "\n",
    "Esse tamanho foi determinado de acordo com a distribuição dos tamanhos das sentenças ao longo do corpus. Para obter um compromisso entre sentenças com muito *padding* e sentenças que devem ter palavras removidas, o tamanho foi definido como aquele que evita eliminar palavras de 75% de todas as sentenças do corpus, incluindo as bases de treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:28.585458Z",
     "start_time": "2018-11-25T05:05:28.484656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49932.000000\n",
       "mean        18.940779\n",
       "std         12.070051\n",
       "min          1.000000\n",
       "25%         10.000000\n",
       "50%         17.000000\n",
       "75%         25.000000\n",
       "max        248.000000\n",
       "Name: words, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences['words'].map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:29.516262Z",
     "start_time": "2018-11-25T05:05:28.589074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sentences['words'].map(len).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:29.631544Z",
     "start_time": "2018-11-25T05:05:29.519491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = int(df_sentences['words'].map(len).describe()['75%'])\n",
    "MAX_SENTENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:29.651563Z",
     "start_time": "2018-11-25T05:05:29.637319Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_sentence(sentence):\n",
    "    tokens_to_fill = int(MAX_SENTENCE_LENGTH - len(sentence))\n",
    "    sentence.extend(['<PAD>']*tokens_to_fill)\n",
    "    \n",
    "    return sentence[:MAX_SENTENCE_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:30.600800Z",
     "start_time": "2018-11-25T05:05:29.654835Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train[\"words\"] = df_train[\"words\"].map(fill_sentence)\n",
    "df_train[\"tags\"] = df_train[\"tags\"].map(fill_sentence)\n",
    "\n",
    "df_test[\"words\"] = df_test[\"words\"].map(fill_sentence)\n",
    "df_test[\"tags\"] = df_test[\"tags\"].map(fill_sentence)\n",
    "\n",
    "df_dev[\"words\"] = df_dev[\"words\"].map(fill_sentence)\n",
    "df_dev[\"tags\"] = df_dev[\"tags\"].map(fill_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:30.710488Z",
     "start_time": "2018-11-25T05:05:30.604930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[salto, sete, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD...</td>\n",
       "      <td>[N, ADJ, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;, &lt;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[o, grande, assunto, da, semana, em, nova, yor...</td>\n",
       "      <td>[ART, ADJ, N, PREP+ART, N, PREP, NPROP, NPROP,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[número, duplo, especial, ,, é, inteirinho, de...</td>\n",
       "      <td>[N, ADJ, ADJ, PU, V, ADJ, PCP, PREP, N, PREP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a, endiabrada, editora, tina, brown, ex, da, ...</td>\n",
       "      <td>[ART, PCP, N, NPROP, NPROP, N, PREP+ART, PU, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[além, das, fotos, de, richard, avedon, ., &lt;PA...</td>\n",
       "      <td>[PREP, PREP+ART, N, PREP, NPROP, NPROP, PU, &lt;P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0  [salto, sete, <PAD>, <PAD>, <PAD>, <PAD>, <PAD...   \n",
       "1  [o, grande, assunto, da, semana, em, nova, yor...   \n",
       "2  [número, duplo, especial, ,, é, inteirinho, de...   \n",
       "3  [a, endiabrada, editora, tina, brown, ex, da, ...   \n",
       "4  [além, das, fotos, de, richard, avedon, ., <PA...   \n",
       "\n",
       "                                                tags  \n",
       "0  [N, ADJ, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>, <P...  \n",
       "1  [ART, ADJ, N, PREP+ART, N, PREP, NPROP, NPROP,...  \n",
       "2  [N, ADJ, ADJ, PU, V, ADJ, PCP, PREP, N, PREP, ...  \n",
       "3  [ART, PCP, N, NPROP, NPROP, N, PREP+ART, PU, N...  \n",
       "4  [PREP, PREP+ART, N, PREP, NPROP, NPROP, PU, <P...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:05:30.768044Z",
     "start_time": "2018-11-25T05:05:30.717967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1997.0\n",
       "mean       25.0\n",
       "std         0.0\n",
       "min        25.0\n",
       "25%        25.0\n",
       "50%        25.0\n",
       "75%        25.0\n",
       "max        25.0\n",
       "Name: words, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['words'].map(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding - Word2Vec\n",
    "A representação das palavras na rede neural foi feita através do modelo word2vec. Para isso, foi escolhido um embedding já treinado em português, produzido pelo grupo NILC em da ICMC USP [4] O embedding escolhido foi treinado através do skip-gram e possui vetores de tamanho 100 para cada palavra.\n",
    "\n",
    "Dois novos tokens precisaram ser adicionados ao embedding. O token `<PAD>` é o token utilizado para realizar o *padding* nas sentenças, enquanto o token `<OOV>` é utilizado para substituir nas sentenças as palavras que não puderam ser encontradas no embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:02.392616Z",
     "start_time": "2018-11-25T05:26:02.388260Z"
    }
   },
   "outputs": [],
   "source": [
    "use_pickle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:23:27.875832Z",
     "start_time": "2018-11-25T05:14:57.927863Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_pickle:\n",
    "    w2v_model = pickle.load(open(paths['word2vecpickle'], 'rb'))\n",
    "else:\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(paths['word2vec'])\n",
    "    embed_size = w2v_model.vectors.shape[1]\n",
    "    # Adiciona vetores extras\n",
    "    w2v_model.add(['<PAD>', '<OOV>'], [[0.1] * embed_size, [0.2] * embed_size])\n",
    "    pickle.dump(w2v_model, open(paths['word2vecpickle'], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:25:38.380310Z",
     "start_time": "2018-11-25T05:25:35.036518Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernardoabreu/.local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hemopa', 0.8358801603317261),\n",
       " ('hemonúcleo', 0.8148629665374756),\n",
       " ('hemoal', 0.7524208426475525),\n",
       " ('hemorio', 0.7506005167961121),\n",
       " ('procon', 0.7349455952644348),\n",
       " ('centrinho', 0.7315340638160706),\n",
       " ('hemoam', 0.7078859806060791),\n",
       " ('incor', 0.7009122371673584),\n",
       " ('detran', 0.7000018358230591),\n",
       " ('poupatempo', 0.6961973905563354)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.similar_by_vector('hemocentro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação da base de dados\n",
    "Para que as sentenças possam ser utilizadas na rede neural, os seus tokens precisam ser convertidos em números. O índice de cada token no embedding word2vec é utilizado para substituir os tokens de entrada. As *tags*, ou classes, usadas na saída usam um índice que estabelece 0 como o índice da classe `<PAD>`, e distribui os índices começando do 1 para as `tags` restantes de forma que as mesmas fiquem ordenadas.\n",
    "\n",
    "Após transformar as classes em índices, é necessário realizar uma codificação one-hot-enconding, para que elas possam ser utilizadas na saída da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:25:53.458672Z",
     "start_time": "2018-11-25T05:25:53.433561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (929608, 100)\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = w2v_model.vectors\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "\n",
    "def word2idx(word):\n",
    "    return w2v_model.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return w2v_model.index2word[idx]\n",
    "\n",
    "\n",
    "def prepare_words(sentences):\n",
    "    sentences_x = np.zeros([len(sentences), MAX_SENTENCE_LENGTH], dtype=np.int32)\n",
    "\n",
    "    oov_index = word2idx('<OOV>')\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, word in enumerate(sentence):\n",
    "            try:\n",
    "                sentences_x[i, t] = word2idx(word)\n",
    "            except KeyError:\n",
    "                sentences_x[i, t] = oov_index\n",
    "    return sentences_x\n",
    "\n",
    "def prepare_tags(tag_sentences, tag2index):\n",
    "    tags_y = np.zeros([len(tag_sentences), MAX_SENTENCE_LENGTH], dtype=np.int32)\n",
    "    for i, sentence in enumerate(tag_sentences):\n",
    "        for t, tag in enumerate(sentence):\n",
    "            tags_y[i, t] = tag2index[tag]\n",
    "    return tags_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:17.323729Z",
     "start_time": "2018-11-25T05:26:13.361112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the train data for LSTM...\n",
      "train_x shape: (37948, 25)\n",
      "\n",
      "Preparing the test data for LSTM...\n",
      "test_x shape: (9987, 25)\n",
      "\n",
      "Preparing the validation data for LSTM...\n",
      "dev_x shape: (1997, 25)\n",
      "\n",
      "Preparing the train tags for LSTM...\n",
      "train_y shape: (37948, 25)\n",
      "\n",
      "Preparing the test data for LSTM...\n",
      "test_y shape: (9987, 25)\n",
      "\n",
      "Preparing the validation data for LSTM...\n",
      "dev_y shape: (1997, 25)\n"
     ]
    }
   ],
   "source": [
    "print('\\nPreparing the train data for LSTM...')\n",
    "train_sentences_X = prepare_words(df_train['words'])\n",
    "print('train_x shape:', train_sentences_X.shape)\n",
    "\n",
    "print('\\nPreparing the test data for LSTM...')\n",
    "test_sentences_X = prepare_words(df_test['words'])\n",
    "print('test_x shape:', test_sentences_X.shape)\n",
    "\n",
    "print('\\nPreparing the validation data for LSTM...')\n",
    "dev_sentences_X = prepare_words(df_dev['words'])\n",
    "print('dev_x shape:', dev_sentences_X.shape)\n",
    "\n",
    "\n",
    "print('\\nPreparing the train tags for LSTM...')\n",
    "train_tags_y = prepare_tags(df_train['tags'], tag2id)\n",
    "print('train_y shape:', train_tags_y.shape)\n",
    "\n",
    "print('\\nPreparing the test data for LSTM...')\n",
    "test_tags_y = prepare_tags(df_test['tags'], tag2id)\n",
    "print('test_y shape:', test_tags_y.shape)\n",
    "\n",
    "print('\\nPreparing the validation data for LSTM...')\n",
    "dev_tags_y = prepare_tags(df_dev['tags'], tag2id)\n",
    "print('dev_y shape:', dev_tags_y.shape)\n",
    "\n",
    "cat_train_tags_y = keras.utils.to_categorical(train_tags_y, num_classes=len(id2tag), dtype='int32')\n",
    "cat_test_tags_y = keras.utils.to_categorical(test_tags_y, num_classes=len(id2tag), dtype='int32')\n",
    "cat_dev_tags_y = keras.utils.to_categorical(dev_tags_y, num_classes=len(id2tag), dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitetura do modelo\n",
    "Redes LSTM bidirecionais são redes LSTM que fazem uma passada em cada direção da sequência de entrada, antes de passar para a próxima camada [5]. Isso permite que a rede leve em consideração tanto o contexto dos tokens que vem antes do token que se quer classificar, quanto o contexto dos tokens que aparecem depois. Esse tipo de rede funciona bem para tarefas de POS tagging com poucas sentenas de treinamento.\n",
    "\n",
    "A arquitetura definida para esse projeto foi uma camada de embedding que utiliza word2vec, seguido de uma camada LSTM bidirecional e uma camada densa.\n",
    "\n",
    "As tags de `<PAD>` não são relevantes para o aprendizado das outras tags, além disso, são fáceis de se acertar. Assim, foi criada uma métrica que ignora a acurácia da tag `<PAD>` durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:24.159728Z",
     "start_time": "2018-11-25T05:26:24.136561Z"
    }
   },
   "outputs": [],
   "source": [
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = keras.backend.argmax(y_true, axis=-1)\n",
    "        y_pred_class = keras.backend.argmax(y_pred, axis=-1)\n",
    "        ignore_mask = keras.backend.cast(\n",
    "            keras.backend.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = keras.backend.cast(\n",
    "            keras.backend.equal(y_true_class, y_pred_class), 'int32') * \\\n",
    "            ignore_mask\n",
    "        accuracy = keras.backend.sum(matches) / \\\n",
    "            keras.backend.maximum(keras.backend.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação da arquitetura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:26.288959Z",
     "start_time": "2018-11-25T05:26:26.284417Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:27.547213Z",
     "start_time": "2018-11-25T05:26:27.534275Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.add(\n",
    "#     keras.layers.Embedding(\n",
    "#         input_dim=len(w2v_model.vocab),\n",
    "#         output_dim=emdedding_size,\n",
    "#         input_length=MAX_SENTENCE_LENGTH,\n",
    "#         weights=[pretrained_weights]\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:28.621208Z",
     "start_time": "2018-11-25T05:26:28.610745Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.add(\n",
    "#     keras.layers.Bidirectional(\n",
    "#         keras.layers.LSTM(lstm_size, return_sequences=True)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# model.add(\n",
    "#     keras.layers.TimeDistributed(\n",
    "#         keras.layers.Dense(output_len)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# model.add(keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:29.741722Z",
     "start_time": "2018-11-25T05:26:29.729015Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=\"adam\",\n",
    "#               metrics=['accuracy', ignore_class_accuracy(tag2id['<PAD>'])])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:30.930614Z",
     "start_time": "2018-11-25T05:26:30.925689Z"
    }
   },
   "outputs": [],
   "source": [
    "# csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "#                                            min_delta=0.001,\n",
    "#                                            patience=4,\n",
    "#                                            verbose=1,\n",
    "#                                            mode='min')\n",
    "# model.fit(train_sentences_X, cat_train_tags_y,\n",
    "#           batch_size=64, epochs=5,\n",
    "#           validation_data=(dev_sentences_X, cat_dev_tags_y),\n",
    "#           callbacks=[csv_logger, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:32.057601Z",
     "start_time": "2018-11-25T05:26:32.045403Z"
    }
   },
   "outputs": [],
   "source": [
    "# scores = model.evaluate(test_X, test_Y)\n",
    "# for metric, score in zip(model.metrics_names, scores):\n",
    "#     print(f\"Test model {metric}: {score*100}\")\n",
    "\n",
    "# scores = model.evaluate(train_X, train_Y)\n",
    "# for metric, score in zip(model.metrics_names, scores):\n",
    "#     print(f\"Train model {metric}: {score*100}\")\n",
    "\n",
    "# scores = model.evaluate(dev_X, dev_Y)\n",
    "# for metric, score in zip(model.metrics_names, scores):\n",
    "#     print(f\"Dev model {metric}: {score*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base de treino utilizada para esse trabalho possui 37948 sentenças anotadas, enquanto a base de teste possui 9987 sentenças e a de validação possui 1997 sentenças. Ao longo de todas essas sentenças, os tokens são classificados em 26 classes, excluindo a classe `<PAD>`. O modelo treinado foi capaz de atingir uma alta acurácia para as bases de treino, teste e validação, respecticamente 98,86%, 94,88% e 95,87%. Esse valor de acurácia corresponde à acurácia calculada desconsiderando a classe `<PAD>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:47.575444Z",
     "start_time": "2018-11-25T05:26:47.191799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>ignore_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026227</td>\n",
       "      <td>0.992354</td>\n",
       "      <td>0.988176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.107023</td>\n",
       "      <td>0.969604</td>\n",
       "      <td>0.948420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.096359</td>\n",
       "      <td>0.972599</td>\n",
       "      <td>0.958644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     # loss       acc  ignore_accuracy\n",
       "0  0.026227  0.992354         0.988176\n",
       "1  0.107023  0.969604         0.948420\n",
       "2  0.096359  0.972599         0.958644"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_data = {'train':0, 'test':1, 'dev':2}\n",
    "scores = pd.read_csv('../src/scores.txt')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A acurácia do modelo ao longo do treino pode ser observada abaixo para a base de treino e validação. É possível observar que um valor alto de acurácia já é atingido logo na primeira época, e a partir desse momento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:26:53.586315Z",
     "start_time": "2018-11-25T05:26:52.021471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "[9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_pred = np.loadtxt('../src/test_predict.txt').astype(int)\n",
    "test_tags_y2 = np.loadtxt('../src/test_y').astype(int)\n",
    "print(test_pred[0])\n",
    "print(test_tags_y[0])\n",
    "print(test_tags_y2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:27:18.494213Z",
     "start_time": "2018-11-25T05:27:18.479955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "print(tag2id['ADJ'])\n",
    "print(id2tag[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:27:39.167429Z",
     "start_time": "2018-11-25T05:27:38.328649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <PAD>       1.00      1.00      1.00     95830\n",
      "         ADJ       0.89      0.92      0.90      6891\n",
      "         ADV       0.89      0.90      0.90      4812\n",
      "      ADV-KS       0.57      0.88      0.69       121\n",
      "         ART       0.99      0.98      0.98     11447\n",
      "         CUR       1.00      1.00      1.00       263\n",
      "          IN       0.63      0.52      0.57       118\n",
      "          KC       0.98      0.97      0.97      3756\n",
      "          KS       0.87      0.86      0.87      2333\n",
      "           N       0.95      0.94      0.95     31999\n",
      "       NPROP       0.87      0.89      0.88     13377\n",
      "         NUM       0.94      0.93      0.93      2298\n",
      "         PCP       0.91      0.95      0.93      2991\n",
      "        PDEN       0.84      0.91      0.88       857\n",
      "        PREP       0.97      0.96      0.97     14531\n",
      "    PREP+ADV       0.86      0.93      0.89        27\n",
      "    PREP+ART       0.98      0.97      0.97      8788\n",
      " PREP+PRO-KS       0.80      0.93      0.86        40\n",
      " PREP+PROADJ       0.99      0.99      0.99       286\n",
      "PREP+PROPESS       0.96      0.99      0.98       106\n",
      " PREP+PROSUB       0.81      0.88      0.85       128\n",
      "      PRO-KS       0.92      0.87      0.89      1956\n",
      "      PROADJ       0.94      0.96      0.95      2911\n",
      "     PROPESS       0.96      0.97      0.97      2546\n",
      "      PROSUB       0.84      0.88      0.86      1360\n",
      "          PU       0.99      0.99      0.99     22017\n",
      "           V       0.99      0.99      0.99     17886\n",
      "\n",
      "   micro avg       0.97      0.97      0.97    249675\n",
      "   macro avg       0.90      0.92      0.91    249675\n",
      "weighted avg       0.97      0.97      0.97    249675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_pred.flatten(), test_tags_y.flatten(), target_names=id2tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:30:49.764841Z",
     "start_time": "2018-11-25T05:30:49.737221Z"
    }
   },
   "outputs": [],
   "source": [
    "def classification_report_df(report):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        row = {}\n",
    "        row_data = line.split()\n",
    "        if not row_data:\n",
    "            break\n",
    "        row['class'] = row_data[0]\n",
    "        row['precision'] = float(row_data[1])\n",
    "        row['recall'] = float(row_data[2])\n",
    "        row['f1_score'] = float(row_data[3])\n",
    "        row['support'] = float(row_data[4])\n",
    "        report_data.append(row)\n",
    "    return pd.DataFrame.from_dict(report_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T05:30:51.854300Z",
     "start_time": "2018-11-25T05:30:51.019090Z"
    }
   },
   "outputs": [],
   "source": [
    "df_result_test = classification_report_df(classification_report(test_pred.flatten(), test_tags_y.flatten(), target_names=id2tag))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "1. https://arxiv.org/pdf/1508.01991.pdf\n",
    "2. http://www.aclweb.org/anthology/Y/Y09/Y09-1013.pdf\n",
    "3. http://nilc.icmc.usp.br/macmorpho/\n",
    "4. http://nilc.icmc.usp.br/embeddings\n",
    "5. http://www.aclweb.org/anthology/P16-2067\n",
    "6. https://nlpforhackers.io/lstm-pos-tagger-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
